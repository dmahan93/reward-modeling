model_path: "dmayhem93/pythia-6B-Summarization-sft"
model_type: "causal"
tokenizer_path: "EleutherAI/gpt-neox-20b"
num_layers_unfrozen: 0.5

train_args:
  output_dir: "/mnt/nvme/home/dakota/ckpts/pythia/synthetic-6B-rm"
  num_train_epochs: 1
  logging_steps: 100
  save_strategy: "epoch"
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  warmup_steps: 100
  weight_decay: 0.01
  learning_rate: 5.0e-6
  save_total_limit: 1
  logging_dir: "./logs"
  fp16: True
  gradient_checkpointing: True

data_path: "dmayhem93/summarization-sft-heirarchical-split1"
trainer_type: "ranked"
order: ["20B", "6B", "1B", "125M"]